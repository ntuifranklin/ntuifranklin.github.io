{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2c0c84-f17d-433f-93a9-a650679fbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"DebtPenny_19930401_20221208.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6365ebc-1ee2-428a-aa82-62c48df626d9",
   "metadata": {},
   "source": [
    "Install needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5100828c-d904-4557-981a-675b57d16046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_datareader\n",
      "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "     |████████████████████████████████| 109 kB 4.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from pandas_datareader) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from pandas_datareader) (2.27.1)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (from pandas_datareader) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (1.21.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_datareader) (1.16.0)\n",
      "Installing collected packages: pandas-datareader\n",
      "Successfully installed pandas-datareader-0.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4982973d-b885-4146-a025-6d5566274af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "     |████████████████████████████████| 588.3 MB 20 kB/s              \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     |████████████████████████████████| 2.4 MB 11.0 MB/s            \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 15.2 MB/s            \n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 6.9 MB/s             \n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     |████████████████████████████████| 124 kB 12.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.19.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "     |████████████████████████████████| 14.1 MB 129 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     |████████████████████████████████| 6.0 MB 2.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 4.6 MB/s             \n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "     |████████████████████████████████| 4.8 MB 10.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (59.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.0.1)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     |████████████████████████████████| 439 kB 9.9 MB/s            \n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 4.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 9.1 MB/s            \n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "     |████████████████████████████████| 177 kB 14.5 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 1.8 MB/s             \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 14.7 MB/s            \n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     |████████████████████████████████| 232 kB 14.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.7)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 13.8 MB/s            \n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.10.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2021.10.8)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 5.8 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.1)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, MarkupSafe, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.0.1\n",
      "    Uninstalling MarkupSafe-2.0.1:\n",
      "      Successfully uninstalled MarkupSafe-2.0.1\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 werkzeug-2.2.2 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281ca770-f5e2-4727-8dd1-72e7e75d41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as reader\n",
    "import datetime as datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18caae11-12d9-4aa5-be7f-11961331b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44dc7b-bb05-48cd-81be-56d06ccdf44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qqq_data = pd.read_csv(\"QQQ.csv\", index_col = \"Date\", parse_dates = True) #Read csv with pandas, parse dates and set date as index column\n",
    "# qqq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4456bf2d-842e-49d9-9a74-757a18bf55c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record Date</th>\n",
       "      <th>Debt Held by the Public</th>\n",
       "      <th>Intragovernmental Holdings</th>\n",
       "      <th>Total Public Debt Outstanding</th>\n",
       "      <th>Source Line Number</th>\n",
       "      <th>Fiscal Year</th>\n",
       "      <th>Fiscal Quarter Number</th>\n",
       "      <th>Calendar Year</th>\n",
       "      <th>Calendar Quarter Number</th>\n",
       "      <th>Calendar Month Number</th>\n",
       "      <th>Calendar Day Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>2.448397e+13</td>\n",
       "      <td>6.826828e+12</td>\n",
       "      <td>3.131080e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>2.452278e+13</td>\n",
       "      <td>6.812309e+12</td>\n",
       "      <td>3.133509e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>2.452552e+13</td>\n",
       "      <td>6.813023e+12</td>\n",
       "      <td>3.133855e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>2.454506e+13</td>\n",
       "      <td>6.807741e+12</td>\n",
       "      <td>3.135280e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-02</td>\n",
       "      <td>2.454628e+13</td>\n",
       "      <td>6.803853e+12</td>\n",
       "      <td>3.135014e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Record Date  Debt Held by the Public  Intragovernmental Holdings  \\\n",
       "0  2022-12-08             2.448397e+13                6.826828e+12   \n",
       "1  2022-12-07             2.452278e+13                6.812309e+12   \n",
       "2  2022-12-06             2.452552e+13                6.813023e+12   \n",
       "3  2022-12-05             2.454506e+13                6.807741e+12   \n",
       "4  2022-12-02             2.454628e+13                6.803853e+12   \n",
       "\n",
       "   Total Public Debt Outstanding  Source Line Number  Fiscal Year  \\\n",
       "0                   3.131080e+13                   1         2023   \n",
       "1                   3.133509e+13                   1         2023   \n",
       "2                   3.133855e+13                   1         2023   \n",
       "3                   3.135280e+13                   1         2023   \n",
       "4                   3.135014e+13                   1         2023   \n",
       "\n",
       "   Fiscal Quarter Number  Calendar Year  Calendar Quarter Number  \\\n",
       "0                      1           2022                        4   \n",
       "1                      1           2022                        4   \n",
       "2                      1           2022                        4   \n",
       "3                      1           2022                        4   \n",
       "4                      1           2022                        4   \n",
       "\n",
       "   Calendar Month Number  Calendar Day Number  \n",
       "0                     12                    8  \n",
       "1                     12                    7  \n",
       "2                     12                    6  \n",
       "3                     12                    5  \n",
       "4                     12                    2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qqq_data = pd.read_csv(dataset) #Read csv with pandas, parse dates and set date as index column\n",
    "qqq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "619ac03a-92df-463f-beae-0604c5096621",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq_data.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ed44da-1401-46cd-b543-a645e8e20f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Record Date', 'Debt Held by the Public', 'Intragovernmental Holdings', 'Total Public Debt Outstanding', 'Source Line Number', 'Fiscal Year', 'Fiscal Quarter Number', 'Calendar Year', 'Calendar Quarter Number', 'Calendar Month Number', 'Calendar Day Number']\n"
     ]
    }
   ],
   "source": [
    "print(qqq_data.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4858616-9fd6-4a2e-bcbc-80f3319a1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(qqq_data[['Debt Held by the Public', 'Intragovernmental Holdings', 'Source Line Number', 'Fiscal Year', 'Fiscal Quarter Number', 'Calendar Year', 'Calendar Quarter Number', 'Calendar Month Number', 'Calendar Day Number']], qqq_data['Total Public Debt Outstanding'], test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6367ca6f-d348-47b2-a198-e58f6134708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Debt Held by the Public  Intragovernmental Holdings  Source Line Number  \\\n",
      "6046                      NaN                         NaN                   1   \n",
      "1782             1.330463e+13                5.227705e+12                   1   \n",
      "2851             9.907753e+12                4.672952e+12                   1   \n",
      "6956                      NaN                         NaN                   1   \n",
      "1375             1.430530e+13                5.540655e+12                   1   \n",
      "...                       ...                         ...                 ...   \n",
      "5191                      NaN                         NaN                   1   \n",
      "5226                      NaN                         NaN                   1   \n",
      "5390                      NaN                         NaN                   1   \n",
      "860              1.620245e+13                5.820374e+12                   1   \n",
      "7270                      NaN                         NaN                   1   \n",
      "\n",
      "      Fiscal Year  Fiscal Quarter Number  Calendar Year  \\\n",
      "6046         1999                      1           1998   \n",
      "1782         2016                      1           2015   \n",
      "2851         2011                      4           2011   \n",
      "6956         1995                      2           1995   \n",
      "1375         2017                      3           2017   \n",
      "...           ...                    ...            ...   \n",
      "5191         2002                      3           2002   \n",
      "5226         2002                      2           2002   \n",
      "5390         2001                      3           2001   \n",
      "860          2019                      4           2019   \n",
      "7270         1994                      1           1993   \n",
      "\n",
      "      Calendar Quarter Number  Calendar Month Number  Calendar Day Number  \n",
      "6046                        4                     11                    4  \n",
      "1782                        4                     11                    3  \n",
      "2851                        3                      8                    2  \n",
      "6956                        1                      3                   16  \n",
      "1375                        2                      6                   19  \n",
      "...                       ...                    ...                  ...  \n",
      "5191                        2                      4                   12  \n",
      "5226                        1                      2                   22  \n",
      "5390                        2                      6                   21  \n",
      "860                         3                      7                    9  \n",
      "7270                        4                     12                   15  \n",
      "\n",
      "[6331 rows x 9 columns]\n",
      "      Debt Held by the Public  Intragovernmental Holdings  Source Line Number  \\\n",
      "1287             1.475222e+13                5.691235e+12                   1   \n",
      "6060                      NaN                         NaN                   1   \n",
      "1978             1.297053e+13                5.124586e+12                   1   \n",
      "3269             7.709912e+12                4.377532e+12                   1   \n",
      "1038             1.582518e+13                5.846300e+12                   1   \n",
      "...                       ...                         ...                 ...   \n",
      "2199             1.253145e+13                4.970129e+12                   1   \n",
      "7274                      NaN                         NaN                   1   \n",
      "5514                      NaN                         NaN                   1   \n",
      "3757             5.120576e+12                4.003440e+12                   1   \n",
      "3583             5.510538e+12                4.157656e+12                   1   \n",
      "\n",
      "      Fiscal Year  Fiscal Quarter Number  Calendar Year  \\\n",
      "1287         2018                      1           2017   \n",
      "6060         1999                      1           1998   \n",
      "1978         2015                      2           2015   \n",
      "3269         2010                      1           2009   \n",
      "1038         2019                      1           2018   \n",
      "...           ...                    ...            ...   \n",
      "2199         2014                      2           2014   \n",
      "7274         1994                      1           1993   \n",
      "5514         2001                      1           2000   \n",
      "3757         2008                      1           2007   \n",
      "3583         2008                      4           2008   \n",
      "\n",
      "      Calendar Quarter Number  Calendar Month Number  Calendar Day Number  \n",
      "1287                        4                     10                   24  \n",
      "6060                        4                     10                   15  \n",
      "1978                        1                      1                   27  \n",
      "3269                        4                     12                    4  \n",
      "1038                        4                     10                   19  \n",
      "...                       ...                    ...                  ...  \n",
      "2199                        1                      3                   11  \n",
      "7274                        4                     12                    9  \n",
      "5514                        4                     12                   21  \n",
      "3757                        4                     12                   27  \n",
      "3583                        3                      9                    4  \n",
      "\n",
      "[1118 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cc60b-5941-47d2-9d54-353b72729dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Model Architecture\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, x_train.shape[1]), activation=\"relu\", return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "#An optimizer is used to help improve upon the Loss function, while the Loss function gives us information on \n",
    "#how the model did on training.\n",
    "lstm.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e48c08-520f-4a91-89a7-be86f70cd00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d0651-7b7f-4475-af11-154c49d4991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.array(x_train)\n",
    "xtrain = xtrain.reshape(xtrain.shape[0], 1, xtrain.shape[1]) #Reshape data to align with parameter\n",
    "#Here we start training the model by setting it to iterate over the entire dataset a 100 times, with a batch size of 8\n",
    "history = lstm.fit(xtrain, y_train, epochs = 100, batch_size = 8, verbose = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac76dd4-304e-4058-a477-f49e2b9a3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to a np array so that we can use it in our LSTM Model.\n",
    "xtest = np.array(x_test)\n",
    "#Now we need to reshape our data because our dataset is 2D but the LSTM Model expects a 3D shape\n",
    "xtest = xtest.reshape(xtest.shape[0], 1, xtest.shape[1]) #Reshape data to align with parameter\n",
    "#Next we need to get the models predicted price values\n",
    "model_pred = lstm.predict(xtest)\n",
    "model_pred;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fac3fd-c537-4db1-9f0b-58996726d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Actual Stock Prices\n",
    "sns.lineplot(data=qqq_data, x=\"Date\", y=\"Adj Close\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044b18c-c9ed-470c-9116-ae4b1951ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Predicted Stock Prices\n",
    "x_test[\"Adj Close\"] = model_pred\n",
    "sns.lineplot(data=x_test, x=\"Date\", y=\"Adj Close\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837fb77-8747-4d25-bf0a-6c8d2aeaf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we load in the data\n",
    "riot_data = pd.read_csv(\"RIOT2016to2019.csv\", index_col = \"Date\", parse_dates = True) #Read csv with pandas, parse dates and set date as index column\n",
    "riot_data.head()\n",
    "\n",
    "#Next we prepare the data\n",
    "scale = MinMaxScaler(feature_range = (0, 1))\n",
    "#Next we reshape the data to be able to fit the liner regression model\n",
    "reshaped_data = riot_data['Close'].values.reshape(-1, 1)\n",
    "scaling_data = scale.fit_transform(reshaped_data)\n",
    "scaling_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2109f14-fb48-435e-b8e1-2a2f041b4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty arrays to store data to train\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "length_of_scale = len(scaling_data)\n",
    "#We want to start prediction from 50 days to the end of the length of the scaled dataset\n",
    "#We are going to fill the data by starting at the 50th index and iterate until the end of the scaled data\n",
    "for values in range(50, length_of_scale):\n",
    "    #Append values to the train dataset array, we are trying to set the model to train the first 50 values and append that to xtrain\n",
    "    #Then from the 51st value that we already know we are going to set that to ytrain so that our model will be ble to predict the next values\n",
    "    xtrain.append(scaling_data[values - 50 : values, 0])\n",
    "    ytrain.append(scaling_data[values, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd076c5-30a4-4352-a208-1a5c63e1e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Next we start convert the data to an array\n",
    "xtrain = np.array(xtrain)\n",
    "#Now we need to reshape our data because our dataset is 2D but the LSTM Model expects a 3D shape just like our previous model\n",
    "xtrain = np.reshape(xtrain, (xtrain.shape[0], xtrain.shape[1], 1))\n",
    "ytrain = np.array(ytrain) #Converting the data to an array\n",
    "#To start building the Model, we will need to couple of things.\n",
    "#We will need to build layers. We are going to create about 7 layers for our model\n",
    "second_model = Sequential()\n",
    "second_model.add(LSTM(40, input_shape = (xtrain.shape[1], 1), return_sequences = True))\n",
    "#The Dropout layer is used to randomly sets input units to 0 with a frequency of rate\n",
    "#at each step during training time. I am adding this so as to helps prevent overfitting.\n",
    "second_model.add(Dropout(0.4))\n",
    "#Adding another lader to the model to aid in prediction\n",
    "second_model.add(LSTM(40, return_sequences = True))\n",
    "#The Dropout layer is used to randomly sets input units to 0 with a frequency of rate\n",
    "#at each step during training time. I am adding this so as to helps prevent overfitting.\n",
    "second_model.add(Dropout(0.4))\n",
    "second_model.add(LSTM(40))\n",
    "second_model.add(Dropout(0.4))\n",
    "#This will serve as the neuron that will predict the closing price value from our data after training\n",
    "second_model.add(Dense(1))\n",
    "#An optimizer is used to help imporove upon the Loss function, while the Loss function gives us information on \n",
    "#how the model did on training.\n",
    "second_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "second_model.fit(xtrain, ytrain, epochs = 30, batch_size = 25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c8a2b-11cd-4617-b988-ab4299710c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Get test data\n",
    "test = pd.read_csv(\"RIOT2019to2021.csv\", index_col = \"Date\", parse_dates = True) #Read csv with pandas, parse dates and set date as index column\n",
    "test.head() #print out \n",
    "right_stock_price = test['Close'].values\n",
    "#all_data = pd.read_csv(\"QQQ\", index_col = \"Date\", parse_dates = True) #Read csv with pandas, parse dates and set date as index column\n",
    "all_data = pd.concat((riot_data['Close'], test['Close']))\n",
    "all_data.head() #print out \n",
    "all_data_length = len(all_data) #get the length so we know where to start\n",
    "test_length = len(test) #get the length so we know where to start and end\n",
    "data_sum = all_data_length - test_length #we know the exact length excess of both data\n",
    "prediction_model = all_data[data_sum - 50 : ].values #we want to go from 1259 - 505 - the number of days to predict which is 50\n",
    "\n",
    "#Next we need to scale it like before and all we are doing here is repeating the process above\n",
    "prediction_model = prediction_model.reshape(-1, 1) #We need to reshape this model to column without changing its data\n",
    "\n",
    "prediction_model_length = len(prediction_model)\n",
    "prediction_model = scale.transform(prediction_model) #We need to transform because initially we scaled it up before. So now we have to turn it back to data we can use\n",
    "#create new testing variable array\n",
    "new_tester = []\n",
    "#Now we can make predictions with all the data we have, which is what we did earlier, we are just repeating the step\n",
    "for values in range(50, prediction_model_length):\n",
    "  #Populate the array\n",
    "  new_tester.append(prediction_model[values - 50 : values, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ada0b9-47fd-4aae-9071-6844c71be7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After populating the array, next we \n",
    "new_tester = np.array(new_tester)\n",
    "#Have to reshape new tester to fit the size of a 3D shape\n",
    "new_tester = np.reshape(new_tester, (new_tester.shape[0], new_tester.shape[1], 1))\n",
    "\n",
    "#After gathering all the info now we can predict the stock prices\n",
    "potential_prices = second_model.predict(new_tester)\n",
    "#potential_prices = scaler.inverse_transform(potential_prices)\n",
    "revert_prices = scale.inverse_transform(potential_prices)\n",
    "revert_prices;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb701c68-04aa-4497-a440-8e17402c5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "plt.plot(right_stock_price, color = 'green')\n",
    "plt.plot(revert_prices, color = 'blue')\n",
    "plt.xlabel(\"TIME SCALE\", fontweight = \"bold\")\n",
    "plt.ylabel(\"IPO PRINCE IN $\", fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2399de-20b2-48e6-a65f-24bc6c82b967",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## LSTM COMPARISON\n",
    "\n",
    "As you can see, this new LSTM model with more layers gave use a better price prediction since the Initial Public Offering \n",
    "of the QQQ Stock. Through Neural Networks, we set up predictions as to what the next price of the stock will be over \n",
    "time using historical data. On our second model, the Blue line serves as our predicted prices over the time scale. \n",
    "The green line on the plot serves as the actual price of the stock over the time scale. \n",
    "I believe it is safe to say our second approximation supersedes the first.\n",
    "\n",
    "## THEOREM\n",
    "\n",
    "We had a theory that the more layers and neurons that we used on our next model, the more successful our approximation \n",
    "would be. Through the stabilization, processing and analysis on our historic data, we created a collection of all the \n",
    "data by merging both year 2016 to 2019 and 2019 up until the present day. We had to create a set of data by gathering \n",
    "all the price fluctuations ever recorded since the stock IPO. That is exactly what we did by merging all the data. With that, \n",
    "we knew all the closing prices every recorded up until the present day. With all this data, we were ready to create the perfect neural network. We had all the data that we needed and we knew that there was a set optimal number of layers and neurons that our model needed to create the perfect configuration. As you can tell by our previous model, some of our predictions were overfitted. On this next model, we solved this problem by providing the model with the exact amount of neurons and layers that it needed to do the job. The result was the Neural Network predicting a better closing price estimation over the time scale.\n",
    "\n",
    "## CONCLUSION\n",
    "\n",
    "In conclusion, with the use of machine learning, we have successfuly analyzed and predicted the price of a stock across\n",
    "specific times. This market prediction was derived with the analysis of Opening Value, Market High per day, Market Low per\n",
    "day and Closing price per day. We also took into consideration the trading volume per day. With all these parameters factored \n",
    "in, we could get a pretty good esimation to what the actual stock prices were using all the above fearture parameters. \n",
    "Although this model yielded a pretty close estimation, the stock prices are steadily affected by the news about the company, \n",
    "activities going on in the country, the economy, news on mergers and collaborations, as well as news on pandemics. \n",
    "These factors can not be predicted ahead of time because of the nature of situations. Through Machine Learning algorithms, \n",
    "technology is able to saves us time and money in the sense that we can spend less time using procedures to compute and analyze \n",
    "market structures, as well as saving us money by creating models to predict the trends of markets which in turn will signify \n",
    "when it is a good time to enter a trade, or signify when to short a stock. Machine learning outperforms humans when it comes\n",
    "to performance and speed. All this is made possible with the analysis of historical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3caaae-1626-45b1-a05a-fe47465c36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "! pwd\n",
    "\n",
    "\n",
    "%%shell\n",
    "jupyter nbconvert --to html ///content/320Final.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
